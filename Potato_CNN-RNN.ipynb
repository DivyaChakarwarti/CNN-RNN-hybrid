{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset with batch size of 32...\n",
      "Found 900 images belonging to 3 classes.\n",
      "Found 300 images belonging to 3 classes.\n",
      "Dataset loaded\n",
      "Building model...\n",
      "Model built\n",
      "Starting training\n",
      "WARNING:tensorflow:From <ipython-input-1-bc5189d75f58>:134: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/40\n",
      "   1/4480 [..............................] - ETA: 0s - loss: 1.1074WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "  29/4480 [..............................] - ETA: 16:12:58 - loss: 1.2355WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 179200 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 5000 batches). You may need to use the repeat() function when building your dataset.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35300, saving model to ./initial_cnn_rnn_weights_2.hdf5\n",
      "  29/4480 [..............................] - 459s 16s/step - loss: 1.2355 - val_loss: 0.3530\n",
      "Initial training done, starting phase two (finetuning)\n",
      "Loading the dataset with batch size of 16...\n",
      "Found 900 images belonging to 3 classes.\n",
      "Found 300 images belonging to 3 classes.\n",
      "Dataset loaded\n",
      "DKYL1\n",
      "DKYL2\n",
      "Epoch 1/40\n",
      "  57/2240 [..............................] - ETA: 11:37:12 - accuracy: 0.9700 - top_k_categorical_accuracy: 1.0000 - loss: 0.1116WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 89600 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 5000 batches). You may need to use the repeat() function when building your dataset.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06020, saving model to ./finetuned_cnn_rnn_weights_2.hdf5\n",
      "  57/2240 [..............................] - 1242s 22s/step - accuracy: 0.9700 - top_k_categorical_accuracy: 1.0000 - loss: 0.1116 - val_loss: 0.0602 - val_top_k_categorical_accuracy: 1.0000 - val_accuracy: 0.9800\n",
      "Training done, doing final evaluation...\n",
      "DKYL3\n",
      "DKYL4\n",
      "WARNING:tensorflow:From <ipython-input-1-bc5189d75f58>:172: Model.evaluate_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.evaluate, which supports generators.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 5000 batches). You may need to use the repeat() function when building your dataset.\n",
      "['loss', 'accuracy', 'top_k_categorical_accuracy'] [0.06020167097449303, 0.9800000190734863, 1.0]\n",
      "accuracy: 98.00%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.engine import Input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Reshape, Lambda, LSTM\n",
    "from keras.layers import  K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "\n",
    "class CustomImageDataGenerator(ImageDataGenerator):\n",
    "    \"\"\"\n",
    "    Because Xception utilizes a custom preprocessing method, the only way to utilize this\n",
    "    preprocessing method using the ImageDataGenerator is to overload the standardize method.\n",
    "\n",
    "    The standardize method gets applied to each batch before ImageDataGenerator yields that batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def standardize(self, x):\n",
    "        \"\"\"\n",
    "        Taken from keras.applications.xception.preprocess_input\n",
    "        \"\"\"\n",
    "        if self.featurewise_center:\n",
    "            x /= 255.\n",
    "            x -= 0.5\n",
    "            x *= 2.\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_training_generator(batch_size=128):\n",
    "    train_data_dir = '/home/divya/Documents/Minor_Project_Paper/CNN-RNN_implement/Potato/Train'\n",
    "    validation_data_dir = '/home/divya/Documents/Minor_Project_Paper/CNN-RNN_implement/Potato/Valid'\n",
    "    image_datagen = CustomImageDataGenerator(featurewise_center=True)\n",
    "\n",
    "    train_generator = image_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_generator = image_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_generator, val_generator\n",
    "\n",
    "\n",
    "def rgb_to_grayscale(input):\n",
    "    \"\"\"Average out each pixel across its 3 RGB layers resulting in a grayscale image\"\"\"\n",
    "    return K.mean(input, axis=3)\n",
    "\n",
    "\n",
    "def rgb_to_grayscale_output_shape(input_shape):\n",
    "    return input_shape[:-1]\n",
    "\n",
    "\n",
    "batch_size_phase_one = 32\n",
    "batch_size_phase_two = 16\n",
    "nb_val_samples = 5000\n",
    "\n",
    "nb_epochs = 40\n",
    "\n",
    "img_width = 299\n",
    "img_height = 299\n",
    "\n",
    "# Setting tensorbord callback\n",
    "now = time.strftime(\"%c\")\n",
    "tensorboard_callback = tensorflow.keras.callbacks.TensorBoard(log_dir='./logs/' + 'cnn_rnn ' + now, histogram_freq=0, write_graph=True,\n",
    "                                   write_images=False)\n",
    "\n",
    "# Loading dataset\n",
    "print(\"Loading the dataset with batch size of {}...\".format(batch_size_phase_one))\n",
    "train_generator, val_generator = get_training_generator(batch_size_phase_one)\n",
    "print(\"Dataset loaded\")\n",
    "\n",
    "print(\"Building model...\")\n",
    "input_tensor = tf.keras.Input(shape=(img_width, img_height, 3))\n",
    "\n",
    "# Creating CNN\n",
    "cnn_model = Xception(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "x = cnn_model.output\n",
    "cnn_bottleneck = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Make CNN layers not trainable\n",
    "for layer in cnn_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Creating RNN\n",
    "x = Lambda(rgb_to_grayscale, rgb_to_grayscale_output_shape)(input_tensor)\n",
    "x = Reshape((23, 3887))(x)  # 23 timesteps, input dim of each timestep 3887\n",
    "x = LSTM(2048, return_sequences=True)(x)\n",
    "rnn_output = LSTM(2048)(x)\n",
    "\n",
    "# Merging both cnn bottleneck and rnn's output wise element wise multiplication\n",
    "x = tf.keras.layers.Concatenate(axis=-1)([cnn_bottleneck, rnn_output])\n",
    "predictions = Dense(3, activation='softmax')(x)  \n",
    "\n",
    "model = tf.keras.Model(inputs=input_tensor, outputs=predictions)\n",
    "\n",
    "print(\"Model built\")\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "print(\"Starting training\")\n",
    "checkpointer = ModelCheckpoint(filepath=\"./initial_cnn_rnn_weights_2.hdf5\", verbose=1, save_best_only=True)\n",
    "\n",
    "'''\n",
    "\n",
    "fit_generator(\n",
    "    generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None,\n",
    "    validation_data=None, =None, validation_freq=1,\n",
    "    class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "    shuffle=True, initial_epoch=0\n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=4480, epochs=nb_epochs, verbose=1,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=5000,\n",
    "                    callbacks=[tensorboard_callback, checkpointer])\n",
    "\n",
    "print(\"Initial training done, starting phase two (finetuning)\")\n",
    "\n",
    "# Load two new generator with smaller batch size, needed because using the same batch size\n",
    "# for the fine tuning will result in GPU running out of memory and tensorflow raising an error\n",
    "print(\"Loading the dataset with batch size of {}...\".format(batch_size_phase_two))\n",
    "train_generator, val_generator = get_training_generator(batch_size_phase_two)\n",
    "print(\"Dataset loaded\")\n",
    "\n",
    "\n",
    "# Load best weights from initial training\n",
    "model.load_weights(\"./initial_cnn_rnn_weights_2.hdf5\")\n",
    "\n",
    "# Make all layers trainable for finetuning\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'top_k_categorical_accuracy'])\n",
    "\n",
    "print(\"DKYL1\")\n",
    "checkpointer = ModelCheckpoint(filepath=\"./finetuned_cnn_rnn_weights_2.hdf5\", verbose=1, save_best_only=True)\n",
    "print(\"DKYL2\")\n",
    "model.fit_generator(train_generator, steps_per_epoch=2240, epochs=nb_epochs, verbose=1,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=5000,\n",
    "                    callbacks=[tensorboard_callback, checkpointer])\n",
    "\n",
    "# Final evaluation of the model\n",
    "print(\"Training done, doing final evaluation...\")\n",
    "\n",
    "print(\"DKYL3\")\n",
    "model.load_weights(\"./finetuned_cnn_rnn_weights_2.hdf5\")\n",
    "print(\"DKYL4\")\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'top_k_categorical_accuracy'])\n",
    "\n",
    "scores = model.evaluate_generator(val_generator, steps=nb_val_samples)\n",
    "print(model.metrics_names, scores)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
